{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 사용 Text 분류 모델 구현\n",
    ": Keras의 Embedding,LSTM,Dropout 계층 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Spam and Ham 분류 테스트 데이터 셋 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 5)\n",
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat... ham\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv', delimiter=',',encoding='latin-1')\n",
    "# ISO/IEC 8859-1, https://ko.wikipedia.org/wiki/ISO/IEC_8859-1\n",
    "print(df.shape)\n",
    "print(df['v2'][0], df['v1'][0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불필요한 컬럼 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['v1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분포를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of ham and spam messages')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df.v1)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Number of ham and spam messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y 값을 0, 1로 변환 : LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] (5572, 1)\n"
     ]
    }
   ],
   "source": [
    "X = df.v2\n",
    "Y = df.v1\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y) # Y 값이 0(ham),1(spam)로 변환, Logistic Regression, sigmoid사용\n",
    "Y = Y.reshape(-1,1)     # 2차원\n",
    "print(Y,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900,) (1672,) (3900, 1) (1672, 1)\n"
     ]
    }
   ],
   "source": [
    "### train 과 test 데이터 셋으로 분리\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3)  # 70:30\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장을 토큰화 처리, 패딩, 데이터셋의 길이를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "# tf.keras.preprocessing.text.Tokenizer(\n",
    "#     num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,\n",
    "#     split=' ', char_level=False, oov_token=None, document_count=0, **kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[106, 238, 40, 81, 12, 723, 21], [57, 25, 57, 18, 83, 79, 599, 8, 398, 845, 82, 166, 271, 8, 83, 399, 31, 263, 724, 11, 535, 20], [933, 12, 846, 846, 12, 725, 725, 12, 272, 272, 12, 6, 25, 6, 12, 10, 68, 2, 46, 28, 199, 10, 32, 6, 56, 10, 32, 6, 33, 80, 116, 6, 77, 847, 9, 18, 6, 87, 121, 116, 2, 726, 152, 6, 77, 679, 18, 217, 41, 6, 188, 8, 344, 2, 6, 264, 14], [285, 81, 443, 34, 34, 48, 18, 82, 400, 239]]\n",
      "(3900, 150)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,  12, 723,  21],\n",
       "       [  0,   0,   0, ...,  11, 535,  20],\n",
       "       [  0,   0,   0, ...,   6, 264,  14],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   7, 254, 119],\n",
       "       [  0,   0,   0, ...,   4,  55,  64],\n",
       "       [  0,   0,   0, ...,  26, 307,  82]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰나이저를 실행하여 단어를 숫자값, 인덱스로 변환하여 저장하고 문장을 벡터로 표현\n",
    "\n",
    "max_words = 1000    # vocab_size , V\n",
    "output_dim = 50     # D , hidden_size(Embedding), input_size(LSTM)\n",
    "max_len = 150       # 문장의 길이 , sequence_length , T\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)  # 가장 빈도가 높은 1000 개의 단어들만 사용하여 토큰화\n",
    "\n",
    "# 단어 인덱스를 구축\n",
    "tokenizer.fit_on_texts(X_train)  # 범위 : 1 ~ 1000\n",
    "\n",
    "# 문자열을 정수 인덱스의 리스트로 변환, (문장을 정수로 인코딩)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)  # type은 list\n",
    "print(sequences[:4])\n",
    "# print(X_train[:4])\n",
    "\n",
    "# word_to_index = tokenizer.word_index\n",
    "# print(word_to_index)\n",
    "# print(len(word_to_index)) # 7380\n",
    "\n",
    "# zero 패딩을 한 벡터 표현을 얻음\n",
    "sequences_matrix = sequence.pad_sequences(sequences, maxlen=max_len) # 신경망에 입력할 X값이다\n",
    "print(sequences_matrix.shape)  # (3900, 150)\n",
    "sequences_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 신경망 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "# tf.keras.layers.Embedding(\n",
    "#     input_dim, output_dim, embeddings_initializer='uniform',\n",
    "#     embeddings_regularizer=None, activity_regularizer=None,\n",
    "#     embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs\n",
    "# )\n",
    "\n",
    "# input_dim : input_dim   , V  ,     max_words : 1000\n",
    "# output_dim : hidden_size , D ,               : 50\n",
    "# input_length : sequence_length , T : max_len : 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers.LSTM(\n",
    "#     units, activation='tanh', recurrent_activation='sigmoid', use_bias=True,\n",
    "#     kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "#     bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None,\n",
    "#     recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "#     kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
    "#     dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False,\n",
    "#     return_state=False, go_backwards=False, stateful=False, time_major=False,\n",
    "#     unroll=False, **kwargs\n",
    "# )\n",
    "\n",
    "# return_sequences=True  이면 3차원(N,T,H)로 출력\n",
    "# return_sequences=False 이면 2차원(N,H)로 출력, 마지막 줄만 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM 1층을 사용하여 신경망을 구현  :  [ Accuracy: 0.984 ]\n",
    "# model = tf.keras.Sequential([\n",
    "#     # X: (N, D)     , W: (V, D)     --> (N,T,D)\n",
    "#     # X: (None,150) , W: (1000,50)  --> (None,150,50)\n",
    "#     tf.keras.layers.Embedding(max_words,output_dim,input_length=max_len),\n",
    "#     tf.keras.layers.LSTM(units=32,return_sequences=True), # 3차원으로 출력 ,(None,150,32)\n",
    "#     tf.keras.layers.Dense(32, activation='relu'),         # 3차원으로 출력 ,(None,150,32)\n",
    "#     tf.keras.layers.Dropout(rate=0.5),                    # 3차원으로 출력 ,(None,150,32)\n",
    "#     tf.keras.layers.Flatten(),                            # 2차원으로 출력 ,(None,150*32)=(None,4800)\n",
    "#     tf.keras.layers.Dense(1,activation='sigmoid')         # 2차원으로 출력 ,(None,1)\n",
    "# ])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 150, 50)           50000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 150, 32)           10624     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 70,033\n",
      "Trainable params: 70,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM 2층을 사용하여 신경망을 구현 :  [ Accuracy: 0.985 ]  # 2층의 정확도가 약간 높다\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_words,output_dim,input_length=max_len),\n",
    "    tf.keras.layers.LSTM(units=32,return_sequences=True), # 3차원으로 출력 ,(None,150,32)\n",
    "    tf.keras.layers.LSTM(units=32),                       # 2차원으로 출력 ,(None,32)\n",
    "    tf.keras.layers.Dense(32, activation='relu'),         # 2차원으로 출력 ,(None,32)\n",
    "    tf.keras.layers.Dropout(rate=0.5),                    # 2차원으로 출력 ,(None,32)\n",
    "    # tf.keras.layers.Flatten(),                         \n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')         # 2차원으로 출력 ,(None,1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "# RMSProp : https://forensics.tistory.com/28\n",
    "# RMSprop 알고리즘은 Adadelta와 마찬가지로 Adagrad에서 학습률이 급격하게 감소하는 문제를 해결 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 0.4532 - accuracy: 0.8417 - val_loss: 0.2636 - val_accuracy: 0.8731\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 3s 109ms/step - loss: 0.2116 - accuracy: 0.9224 - val_loss: 0.1323 - val_accuracy: 0.9667\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 3s 103ms/step - loss: 0.1108 - accuracy: 0.9798 - val_loss: 0.1198 - val_accuracy: 0.9679\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 3s 101ms/step - loss: 0.0692 - accuracy: 0.9853 - val_loss: 0.0848 - val_accuracy: 0.9795\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 2s 100ms/step - loss: 0.0567 - accuracy: 0.9869 - val_loss: 0.0622 - val_accuracy: 0.9833\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 3s 101ms/step - loss: 0.0462 - accuracy: 0.9904 - val_loss: 0.0763 - val_accuracy: 0.9833\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 3s 109ms/step - loss: 0.0315 - accuracy: 0.9920 - val_loss: 0.1230 - val_accuracy: 0.9795\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 3s 105ms/step - loss: 0.0309 - accuracy: 0.9923 - val_loss: 0.0957 - val_accuracy: 0.9833\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 3s 104ms/step - loss: 0.0189 - accuracy: 0.9968 - val_loss: 0.0711 - val_accuracy: 0.9833\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 3s 107ms/step - loss: 0.0174 - accuracy: 0.9965 - val_loss: 0.0908 - val_accuracy: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x249574b6850>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "model.fit(sequences_matrix,Y_train, batch_size=128, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1672, 150) (1672, 1)\n"
     ]
    }
   ],
   "source": [
    "#  Test 데이터셋의 벡터를 구함\n",
    "\n",
    "# 문자열을 정수 인덱스의 리스트로 변환, (문장을 정수로 인코딩)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)  # type은 list\n",
    "\n",
    "# zero 패딩을 한 벡터 표현을 얻음\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences, maxlen=max_len) \n",
    "print(test_sequences_matrix.shape, Y_test.shape)  # (1672, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 15ms/step - loss: 0.0883 - accuracy: 0.9850\n"
     ]
    }
   ],
   "source": [
    "# 정확도\n",
    "accr = model.evaluate(test_sequences_matrix, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Loss: 0.088\n",
      "  Accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.540048e-05]] [0]\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "preds = model.predict(test_sequences_matrix[1].reshape(1,-1))\n",
    "print(preds,Y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0, 621, 789,  95, 272, 845,  47, 250,   8,   9,  14,  25,\n",
       "        14, 185,  47, 250, 989,  14, 930])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.328391e-05]] [0]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_sequences_matrix[17].reshape(1,-1))\n",
    "print(preds,Y_test[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   4,    5,    8,   13,   21,   27,   28,   35,   59,   65,   67,\n",
       "         70,   74,   85,   89,  103,  113,  118,  125,  132,  144,  145,\n",
       "        151,  154,  156,  170,  173,  187,  205,  220,  231,  243,  249,\n",
       "        250,  251,  254,  257,  260,  267,  269,  272,  276,  281,  294,\n",
       "        331,  350,  357,  360,  364,  372,  373,  377,  378,  385,  404,\n",
       "        412,  431,  446,  449,  464,  469,  472,  477,  480,  494,  496,\n",
       "        499,  504,  505,  511,  527,  536,  557,  569,  573,  579,  594,\n",
       "        603,  604,  619,  628,  629,  633,  643,  649,  654,  659,  660,\n",
       "        661,  671,  674,  685,  691,  701,  706,  708,  709,  736,  737,\n",
       "        739,  743,  745,  747,  750,  753,  762,  764,  769,  771,  781,\n",
       "        792,  793,  798,  806,  820,  823,  825,  836,  840,  850,  855,\n",
       "        856,  870,  871,  881,  902,  921,  930,  944,  945,  946,  948,\n",
       "        955,  960,  982,  985,  986,  987,  998, 1015, 1017, 1018, 1023,\n",
       "       1039, 1059, 1068, 1075, 1081, 1082, 1092, 1104, 1106, 1109, 1114,\n",
       "       1119, 1131, 1133, 1134, 1140, 1141, 1149, 1151, 1159, 1164, 1171,\n",
       "       1177, 1180, 1193, 1196, 1206, 1209, 1218, 1235, 1239, 1246, 1251,\n",
       "       1252, 1254, 1257, 1262, 1281, 1282, 1294, 1296, 1297, 1298, 1300,\n",
       "       1314, 1316, 1320, 1329, 1331, 1332, 1367, 1380, 1391, 1403, 1432,\n",
       "       1435, 1438, 1444, 1458, 1459, 1462, 1467, 1469, 1474, 1481, 1483,\n",
       "       1500, 1507, 1511, 1525, 1527, 1539, 1560, 1567, 1573, 1580, 1581,\n",
       "       1585, 1592, 1608, 1643, 1647, 1654, 1667, 1671], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = np.where(Y_test==1)  # 답이 1인 인덱스\n",
    "ones[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9992803]] [1]\n",
      "[[0.99898076]] [1]\n",
      "[[0.9765587]] [1]\n",
      "[[0.9991563]] [1]\n",
      "[[0.9987993]] [1]\n",
      "[[0.9993839]] [1]\n",
      "[[0.99938595]] [1]\n",
      "[[0.9993429]] [1]\n",
      "[[0.9992351]] [1]\n",
      "[[0.9989561]] [1]\n",
      "[[0.9989613]] [1]\n",
      "[[0.99934566]] [1]\n",
      "[[0.99841785]] [1]\n",
      "[[0.9994295]] [1]\n",
      "[[0.9987035]] [1]\n",
      "[[0.99177617]] [1]\n",
      "[[0.99936473]] [1]\n",
      "[[0.9992603]] [1]\n",
      "[[0.9990052]] [1]\n",
      "[[0.99814874]] [1]\n",
      "[[0.9993796]] [1]\n",
      "[[0.99933696]] [1]\n",
      "[[0.9993174]] [1]\n",
      "[[0.99925745]] [1]\n",
      "[[0.9991669]] [1]\n",
      "[[0.9993311]] [1]\n",
      "[[0.992548]] [1]\n",
      "[[0.9994]] [1]\n",
      "[[0.992548]] [1]\n",
      "[[0.07314533]] [1]\n",
      "[[0.99877214]] [1]\n",
      "[[0.00482464]] [1]\n",
      "[[0.9994063]] [1]\n",
      "[[0.9923465]] [1]\n",
      "[[2.6318248e-05]] [1]\n",
      "[[0.99927765]] [1]\n",
      "[[0.99936676]] [1]\n",
      "[[0.99364203]] [1]\n",
      "[[0.99905187]] [1]\n",
      "[[0.99844927]] [1]\n",
      "[[0.9989385]] [1]\n",
      "[[0.9376961]] [1]\n",
      "[[0.99816316]] [1]\n",
      "[[0.9687518]] [1]\n",
      "[[0.99936795]] [1]\n",
      "[[0.99903584]] [1]\n",
      "[[0.9994186]] [1]\n",
      "[[0.99928963]] [1]\n",
      "[[0.99936366]] [1]\n",
      "[[0.99927986]] [1]\n",
      "[[0.58557886]] [1]\n",
      "[[0.9992217]] [1]\n",
      "[[0.9993171]] [1]\n",
      "[[0.9990705]] [1]\n",
      "[[0.9990771]] [1]\n",
      "[[0.2131888]] [1]\n",
      "[[0.9990802]] [1]\n",
      "[[0.99896735]] [1]\n",
      "[[0.99934936]] [1]\n",
      "[[0.9993816]] [1]\n",
      "[[0.99757004]] [1]\n",
      "[[0.79667777]] [1]\n",
      "[[0.9992144]] [1]\n",
      "[[0.992548]] [1]\n",
      "[[0.9982259]] [1]\n",
      "[[0.998475]] [1]\n",
      "[[0.00782454]] [1]\n",
      "[[0.9953363]] [1]\n",
      "[[0.9993392]] [1]\n",
      "[[4.2241303e-05]] [1]\n",
      "[[0.99902433]] [1]\n",
      "[[0.99892366]] [1]\n",
      "[[0.9992894]] [1]\n",
      "[[0.99820554]] [1]\n",
      "[[0.9992635]] [1]\n",
      "[[0.9993955]] [1]\n",
      "[[0.8207216]] [1]\n",
      "[[0.99745846]] [1]\n",
      "[[0.99593973]] [1]\n",
      "[[0.97009915]] [1]\n",
      "[[0.9988253]] [1]\n",
      "[[0.9993745]] [1]\n",
      "[[0.997537]] [1]\n",
      "[[0.9993254]] [1]\n",
      "[[0.999261]] [1]\n",
      "[[0.99936795]] [1]\n",
      "[[0.00083396]] [1]\n",
      "[[0.9976809]] [1]\n",
      "[[0.9991528]] [1]\n",
      "[[0.9993403]] [1]\n",
      "[[0.9992093]] [1]\n",
      "[[0.9988586]] [1]\n",
      "[[0.9953997]] [1]\n",
      "[[0.998986]] [1]\n",
      "[[0.9992839]] [1]\n",
      "[[0.9990565]] [1]\n",
      "[[1.0798124e-05]] [1]\n",
      "[[0.9989685]] [1]\n",
      "[[0.99909955]] [1]\n",
      "[[0.9994652]] [1]\n",
      "[[0.00024104]] [1]\n",
      "[[0.9993144]] [1]\n",
      "[[0.9971738]] [1]\n",
      "[[0.9994063]] [1]\n",
      "[[0.998868]] [1]\n",
      "[[0.9991677]] [1]\n",
      "[[0.9988536]] [1]\n",
      "[[0.99934566]] [1]\n",
      "[[0.99757934]] [1]\n",
      "[[0.19655305]] [1]\n",
      "[[0.9993807]] [1]\n",
      "[[0.99936473]] [1]\n",
      "[[0.99942356]] [1]\n",
      "[[0.99927604]] [1]\n",
      "[[0.998986]] [1]\n",
      "[[0.99893314]] [1]\n",
      "[[0.99940735]] [1]\n",
      "[[0.9990734]] [1]\n",
      "[[0.00032067]] [1]\n",
      "[[0.99932456]] [1]\n",
      "[[0.998492]] [1]\n",
      "[[0.99400556]] [1]\n",
      "[[0.9992863]] [1]\n",
      "[[0.9988731]] [1]\n",
      "[[0.99871635]] [1]\n",
      "[[0.99907005]] [1]\n",
      "[[0.9990163]] [1]\n",
      "[[0.99879193]] [1]\n",
      "[[0.9990705]] [1]\n",
      "[[0.9993581]] [1]\n",
      "[[0.998475]] [1]\n",
      "[[0.99875855]] [1]\n",
      "[[0.9992641]] [1]\n",
      "[[0.9985587]] [1]\n",
      "[[0.9992826]] [1]\n",
      "[[0.9992636]] [1]\n",
      "[[0.996958]] [1]\n",
      "[[0.99877214]] [1]\n",
      "[[0.99930805]] [1]\n",
      "[[0.999226]] [1]\n",
      "[[0.9991892]] [1]\n",
      "[[0.99911106]] [1]\n",
      "[[5.9439662e-05]] [1]\n",
      "[[0.9992863]] [1]\n",
      "[[0.9931898]] [1]\n",
      "[[0.99889016]] [1]\n",
      "[[0.9988581]] [1]\n",
      "[[0.999282]] [1]\n",
      "[[0.9504745]] [1]\n",
      "[[0.9989538]] [1]\n",
      "[[0.9986979]] [1]\n",
      "[[0.9992913]] [1]\n",
      "[[0.9981845]] [1]\n",
      "[[0.40514326]] [1]\n",
      "[[0.9984434]] [1]\n",
      "[[0.99918735]] [1]\n",
      "[[0.99109495]] [1]\n",
      "[[0.9993588]] [1]\n",
      "[[0.9987391]] [1]\n",
      "[[0.99870974]] [1]\n",
      "[[0.99864405]] [1]\n",
      "[[0.99905694]] [1]\n",
      "[[0.99729764]] [1]\n",
      "[[0.9994212]] [1]\n",
      "[[0.99947524]] [1]\n",
      "[[0.6576915]] [1]\n",
      "[[0.03489801]] [1]\n",
      "[[0.99841785]] [1]\n",
      "[[0.999334]] [1]\n",
      "[[0.99932456]] [1]\n",
      "[[0.99933225]] [1]\n",
      "[[2.5745463e-05]] [1]\n",
      "[[0.9987391]] [1]\n",
      "[[0.9985298]] [1]\n",
      "[[0.9992785]] [1]\n",
      "[[0.99768865]] [1]\n",
      "[[0.9954493]] [1]\n",
      "[[0.99400556]] [1]\n",
      "[[0.9801289]] [1]\n",
      "[[0.99940825]] [1]\n",
      "[[0.9993612]] [1]\n",
      "[[0.9991157]] [1]\n",
      "[[0.9994621]] [1]\n",
      "[[0.9994063]] [1]\n",
      "[[0.9990207]] [1]\n",
      "[[0.99826396]] [1]\n",
      "[[0.9504745]] [1]\n",
      "[[0.06939724]] [1]\n",
      "[[0.00059021]] [1]\n",
      "[[0.9989184]] [1]\n",
      "[[0.9985587]] [1]\n",
      "[[0.9988699]] [1]\n",
      "[[0.99922836]] [1]\n",
      "[[0.99929357]] [1]\n",
      "[[0.99889505]] [1]\n",
      "[[0.99935234]] [1]\n",
      "[[0.9993973]] [1]\n",
      "[[0.9992863]] [1]\n",
      "[[0.99912643]] [1]\n",
      "[[0.99933565]] [1]\n",
      "[[0.99841785]] [1]\n",
      "[[0.99934447]] [1]\n",
      "[[0.99930805]] [1]\n",
      "[[0.9993464]] [1]\n",
      "[[0.00168499]] [1]\n",
      "[[7.222966e-05]] [1]\n",
      "[[0.99912375]] [1]\n",
      "[[0.9953753]] [1]\n",
      "[[0.999261]] [1]\n",
      "[[0.00103199]] [1]\n",
      "[[0.99914086]] [1]\n",
      "[[0.9991077]] [1]\n",
      "[[0.9993512]] [1]\n",
      "[[0.99802905]] [1]\n",
      "[[0.9965138]] [1]\n",
      "[[0.99899477]] [1]\n",
      "[[0.8127396]] [1]\n",
      "[[0.99930805]] [1]\n",
      "[[0.66129625]] [1]\n",
      "[[0.99905556]] [1]\n",
      "[[0.99937975]] [1]\n",
      "[[0.99886996]] [1]\n",
      "[[0.9990655]] [1]\n",
      "[[0.99898076]] [1]\n",
      "[[0.99764663]] [1]\n",
      "[[0.9966628]] [1]\n",
      "[[0.9994]] [1]\n",
      "[[0.99928695]] [1]\n"
     ]
    }
   ],
   "source": [
    "for one in ones[0]:\n",
    "    preds = model.predict(test_sequences_matrix[one].reshape(1,-1))\n",
    "    print(preds,Y_test[one])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
