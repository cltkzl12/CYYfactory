{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 사용 문장 생성 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_layers import softmax,Rnnlm,BetterRnnlm,RnnlmTrainer\n",
    "import numpy as np\n",
    "from dataset import ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnlmGen(Rnnlm): # Rnnlm class를 상속 받아 사용\n",
    "    def generate(self,start_id,skip_ids=None,sample_size=100): # sample_size:샘플링하는 단어 수\n",
    "        word_ids = [start_id]\n",
    "        \n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1,1)\n",
    "            score = self.predict(x)      # 3차원\n",
    "            p = softmax(score.flatten()) # 10000개의 단어의 각각의 확률을 구함\n",
    "            \n",
    "            sampled = np.random.choice(len(p),size=1,p=p)\n",
    "            # 확률 분포를 사용하여 random으로 1개의 단어 샘플링, 확률적 방법\n",
    "            \n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x)) # word_ids 리스트에 샘플링된 단어를 추가\n",
    "                \n",
    "        return word_ids\n",
    "                \n",
    "    def get_state(self):\n",
    "        return self.lstm_layer.h, self.lstm_layer.c\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.lstm_layer.set_state(*state)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 929589\n",
      "316\n",
      "[27, 26, 416]\n",
      "you low-income years syndrome dataproducts missile philadelphia remarkable terminal intensify encountered colonial lies proving goodyear label send accrued flashy ill. peaked rarely fitness bank time redford hutton breakers notified eroding advantages feeds wives another confirm wheels batibot unavailable movies peddling morale l. collar barney south iranian shrank across ssangyong trading occurred outside i relied alaska burns 20-year pasadena luxury convert school photography discouraging offshore die differ destroying complaints vaccine mentioned smallest lay ago builds foreigners fad undoubtedly machinists non-violent accountability incorporated aer fully donuts invites nonperforming prescription greece female tariffs crash bikers novels blocked create best-known energy fiscal respected set\n"
     ]
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)  # 10000\n",
    "corpus_size = len(corpus)\n",
    "print(vocab_size,corpus_size)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params('Rnnlm.pkl')  # 미리 학습된 parameter를 읽어오기, 학습 불필요\n",
    "\n",
    "# start 단어와 skip 단어(문자열) 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "print(start_id)  # 316\n",
    "\n",
    "skip_words = ['N','<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]  # 전처리된 단어를 제외\n",
    "print(skip_ids)\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id,skip_ids,100)\n",
    "# 시작할 단어의 id와 제외할 단어 id를 입력하여 100개의 단어 샘플링\n",
    "\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>','.\\n')    # 100개의 단어를 한 문장으로 연결\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 좋은 문장으로 : 2층 LSTM,  Dropout, 가중치 공유 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterRnnlmGen(BetterRnnlm): # BetterRnnlm class를 상속 받아 사용\n",
    "    def generate(self,start_id,skip_ids=None,sample_size=100): # sample_size:샘플링하는 단어 수\n",
    "        word_ids = [start_id]             # start_id : 최초로 시작할 단어\n",
    "        \n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1,1)\n",
    "            score = self.predict(x)       # 3차원\n",
    "            p = softmax(score.flatten())  # 10000개의 단어의 각각의 확률을 구함\n",
    "            # print(p.shape)    # (10000,)\n",
    "            \n",
    "            sampled = np.random.choice(len(p),size=1,p=p)\n",
    "            # 확률 분포를 사용하여 random으로 1개의 단어 샘플링, 확률적 방법\n",
    "            \n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x)) # word_ids 리스트에 샘플링된 단어를 추가\n",
    "        return word_ids\n",
    "\n",
    "    def get_state(self):\n",
    "        states = []\n",
    "        for layer in self.lstm_layers:\n",
    "            states.append((layer.h, layer.c))\n",
    "        return states\n",
    "\n",
    "    def set_state(self, states):\n",
    "        for layer, state in zip(self.lstm_layers, states):\n",
    "            layer.set_state(*state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 929589\n",
      "316\n",
      "[27, 26, 416]\n",
      "you fireman precious professor application national reporting winter airplanes vicar fairness woods interest sassy lucky dark until cell views downward born prospectus representatives compact adjusted brewer drafted removal s&ls fierce optimism rampant nikko exist redevelopment there fletcher pace forcing kelly deadline evasion stakes acknowledging competition pain abused barron projections runaway francs detergent lovely retain circle accessories deficits debt supported lawn angels remember nasa whittington enacted national portable chefs disobedience effective original victim toward places write-offs climbed greece tritium customs elections winning gaubert scripts wilfred triggered food driver malcolm differ say rudolph squibb regain squibb low-cost protected expectation discovision courtaulds surprising\n"
     ]
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "print(vocab_size,corpus_size)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params('BetterRnnlm.pkl')  # 미리 학습된 parameter를 읽어오기\n",
    "\n",
    "# start 단어와 skip 단어(문자열) 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "print(start_id)  # 316\n",
    "\n",
    "skip_words =['N','<unk>','$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]  # 전처리된 단어를 제외\n",
    "print(skip_ids)\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id,skip_ids,100) \n",
    "# 시작할 단어의 id와 제외할 단어 id를 입력하여 100개의 단어 샘플링\n",
    "\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>','.\\n')  # 100개의 단어를 한 문장으로 연결\n",
    "print(txt)  # 실행시 마다 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어열을 초기 값으로 주고 문장을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 4748, 42, 2262, 40]\n",
      "the meaning of life is container fournier soliciting new-issue trendy everything bargain-hunting insider closer window urged maturities clinical hall shed training romantic fcc reflecting bring compensate virtue prediction approached buried aspect piano paid trinova zone underwriting financially sides manage 12-month have spurred greedy proposal depositary affluent quotations due wcrs infected owes vintage harper virus pbs coaches guinness deep sole demise succeeded movements injuries unity escrow jolt fruit complicated academy minimal quotas improves sister confederation cuban rosenthal noncallable pitches looked undo cemetery reassuring coordinate olympics stems anti-nuclear king managua gives anderson championship plaintiff seems those presumed matthews high-definition stolen opponents concept 30-share done beings demler\n"
     ]
    }
   ],
   "source": [
    "model.reset_state()\n",
    "\n",
    "start_words = 'the meaning of life is'\n",
    "start_ids = [word_to_id[w] for w in start_words.split(' ')]\n",
    "print(start_ids)\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_ids[-1], skip_ids)  # 마지막 단어('is')를 시작 단어로 문장 생성\n",
    "word_ids = start_ids[:-1] + word_ids                # 'is' 앞까지의 단어를 앞부분에 추가\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print(txt)  #  실행시 마다 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.f.\n",
      "spin\n",
      "lovely\n",
      "showtime\n"
     ]
    }
   ],
   "source": [
    "# 'the meaning of life' 부분 예측  :  'meaning of life is' 으로 예측 되지 않음\n",
    "for x in start_ids[:-1]:\n",
    "    x = np.array(x).reshape(1, 1)\n",
    "    score = model.predict(x).flatten()\n",
    "    p = softmax(score).flatten()\n",
    "    sampled = np.random.choice(len(p), size=1, p=p)\n",
    "    print(id_to_word[sampled[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
