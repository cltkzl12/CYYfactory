{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#데이터를 분석의 성능을 높이기위해 분산처리인 yarn을 적용\n",
    "spark = SparkSession.builder.master(\"yarn\").\\\n",
    "\tappName(\"mypyspark\").getOrCreate()\n",
    "\n",
    "#모두 하둡파일 시스템의 경로로 지정해 두었습니다.\n",
    "ProductFilePath  = \"hdfs:///user/s21410781/finance/01.Product.csv\"\n",
    "Search1FilePath  = \"hdfs:///user/s21410781/finance/02.Search1.csv\"\n",
    "Search2FilePath  = \"hdfs:///user/s21410781/finance/03.Search.csv\"\n",
    "CustomFilePath  = \"hdfs:///user/s21410781/finance/04.Custom.csv\"\n",
    "SessionFilePath  = \"hdfs:///user/s21410781/finance/05.Session.csv\"\n",
    "MasterFilePath  = \"hdfs:///user/s21410781/finance/06.Master.csv\"\n",
    "\n",
    "# csv파일을 파이썬으로 읽어옵니다.\n",
    "Product = spark.read.csv(ProductFilePath, header='true', sep=',')\n",
    "Product.createOrReplaceTempView(\"product\")\n",
    "\n",
    "Search1 = spark.read.csv(Search1FilePath, header='true', sep=',')\n",
    "Search1.createOrReplaceTempView(\"search1\")\n",
    "\n",
    "Search2 = spark.read.csv(Search2FilePath, header='true', sep=',')\n",
    "Search2.createOrReplaceTempView(\"search2\")\n",
    "\n",
    "Custom = spark.read.csv(CustomFilePath, header='true', sep=',')\n",
    "Custom.createOrReplaceTempView(\"custom\")\n",
    "\n",
    "Session = spark.read.csv(SessionFilePath, header='true', sep=',')\n",
    "Session.createOrReplaceTempView(\"session\")\n",
    "\n",
    "Master = spark.read.csv(MasterFilePath, header='true', sep=',')\n",
    "Master.createOrReplaceTempView(\"master\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search2파일의 검색수를 문자열 -> 숫자로 바꿈 (과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터가 숫자의 데이터여야할것이 문자열로 저장이 되어있었어서 숫자로 바꾸어주는것 (ex. 23,000 -> 230000)\n",
    "search = spark.sql(\"\"\"\n",
    "            select SESS_DT ,KWD_NM ,replace(SEARCH_CNT,',','') as SEARCH_CNT \n",
    "            from search2   \n",
    "            \"\"\")\n",
    "\n",
    "search.createOrReplaceTempView(\"search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search파일의 일별-> 월별로 고침(과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#월별로 분석을 위해서 월별로 고침\n",
    "search_1 = spark.sql(\"\"\"\n",
    "            select substr(SESS_DT,3,4) as SESS_DT ,KWD_NM ,sum(SEARCH_CNT) as SEARCH_CNT \n",
    "            from search\n",
    "            group by SESS_DT,KWD_NM\n",
    "            order by SESS_DT,SEARCH_CNT\"\"\")\n",
    "\n",
    "search_1.createOrReplaceTempView(\"search_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 월별 전체 검색어량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|SESS_DT| SEARCH_CNT|\n",
      "+-------+-----------+\n",
      "|   1808|1.0128466E7|\n",
      "|   1805|1.1737245E7|\n",
      "|   1804|1.1481214E7|\n",
      "|   1809|   1.1089E7|\n",
      "|   1806| 1.125495E7|\n",
      "|   1807|1.1559653E7|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_3 = spark.sql(\"\"\"\n",
    "            select SESS_DT ,round(sum(SEARCH_CNT),0) as SEARCH_CNT\n",
    "            from search_2\n",
    "            group by SESS_DT\"\"\")\n",
    "\n",
    "search_3.createOrReplaceTempView(\"search_3\")\n",
    "p_search_3=search_3.toPandas()\n",
    "p_search_3.to_csv(\"p_search_3.csv\",mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 월별 검색어별 수량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_2 = spark.sql(\"\"\"\n",
    "            select SESS_DT ,KWD_NM ,sum(SEARCH_CNT) as SEARCH_CNT\n",
    "            from search_1\n",
    "            group by SESS_DT,KWD_NM\n",
    "            order by SESS_DT,SEARCH_CNT desc\"\"\")\n",
    "\n",
    "search_2.createOrReplaceTempView(\"search_2\")\n",
    "\n",
    "p_search_2=search_2.toPandas()\n",
    "p_search_2.to_csv(\"p_search_2.csv\",mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4,5,6,7,8,9월 검색어 TOP20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top20_4=spark.sql(\"\"\"\n",
    "            select SESS_DT ,KWD_NM ,SEARCH_CNT\n",
    "            from search_2\n",
    "            where SESS_DT=1804\n",
    "            order by SEARCH_CNT desc\n",
    "            limit 20\"\"\")\n",
    "\n",
    "Top20_4.createOrReplaceTempView(\"Top20_4\")\n",
    "p_Top20_4=Top20_4.toPandas()\n",
    "p_Top20_4.to_csv(\"p_Top20_4.csv\",mode=\"w\")\n",
    "\n",
    "Top20_5=spark.sql(\"\"\"\n",
    "            select SESS_DT ,KWD_NM ,SEARCH_CNT\n",
    "            from search_2\n",
    "            where SESS_DT=1805\n",
    "            order by SEARCH_CNT desc\n",
    "            limit 20\"\"\")\n",
    "\n",
    "Top20_5.createOrReplaceTempView(\"Top20_5\")\n",
    "\n",
    "p_Top20_5=Top20_5.toPandas()\n",
    "p_Top20_5.to_csv(\"p_Top20_5.csv\",mode=\"w\")\n",
    "\n",
    "Top20_6=spark.sql(\"\"\"\n",
    "            select SESS_DT ,KWD_NM ,SEARCH_CNT\n",
    "            from search_2\n",
    "            where SESS_DT=1806\n",
    "            order by SEARCH_CNT desc\n",
    "            limit 20\"\"\")\n",
    "\n",
    "Top20_6.createOrReplaceTempView(\"Top20_6\")\n",
    "p_Top20_6=Top20_6.toPandas()\n",
    "p_Top20_6.to_csv(\"p_Top20_6.csv\",mode=\"w\")\n",
    "\n",
    "Top20_7=spark.sql(\"\"\"\n",
    "            select SESS_DT ,KWD_NM ,SEARCH_CNT\n",
    "            from search_2\n",
    "            where SESS_DT=1807\n",
    "            order by SEARCH_CNT desc\n",
    "            limit 20\"\"\")\n",
    "\n",
    "Top20_7.createOrReplaceTempView(\"Top20_7\")\n",
    "p_Top20_7=Top20_7.toPandas()\n",
    "p_Top20_7.to_csv(\"p_Top20_7.csv\",mode=\"w\")\n",
    "\n",
    "Top20_8=spark.sql(\"\"\"\n",
    "            select SESS_DT ,KWD_NM ,SEARCH_CNT\n",
    "            from search_2\n",
    "            where SESS_DT=1808\n",
    "            order by SEARCH_CNT desc\n",
    "            limit 20\"\"\")\n",
    "\n",
    "Top20_8.createOrReplaceTempView(\"Top20_8\")\n",
    "\n",
    "p_Top20_8=Top20_8.toPandas()\n",
    "p_Top20_8.to_csv(\"p_Top20_8.csv\",mode=\"w\")\n",
    "\n",
    "Top20_9=spark.sql(\"\"\"\n",
    "            select SESS_DT ,KWD_NM ,SEARCH_CNT\n",
    "            from search_2\n",
    "            where SESS_DT=1809\n",
    "            order by SEARCH_CNT desc\n",
    "            limit 20\"\"\")\n",
    "\n",
    "Top20_9.createOrReplaceTempView(\"Top20_9\")\n",
    "\n",
    "p_Top20_9=Top20_9.toPandas()\n",
    "p_Top20_9.to_csv(\"p_Top20_9.csv\",mode=\"w\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이월된 TOP검색어에서 없는것들 추출과정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "top4_5=spark.sql(\"\"\"\n",
    "            select a.KWD_NM as KWD_NM1 ,a.SEARCH_CNT as SEARCH_CNT1 , b.KWD_NM as KWD_NM2  ,b.SEARCH_CNT as SEARCH_CNT2\n",
    "            from top20_4 as a right outer join top20_5 as b on a.KWD_NM = b.KWD_NM \n",
    "            \"\"\")\n",
    "\n",
    "top4_5.createOrReplaceTempView(\"top4_5\")\n",
    "\n",
    "p_top4_5=top4_5.toPandas()\n",
    "p_top4_5.to_csv(\"p_top4_5.csv\",mode=\"w\")\n",
    "\n",
    "top5_6=spark.sql(\"\"\"\n",
    "            select a.KWD_NM as KWD_NM1 ,a.SEARCH_CNT as SEARCH_CNT1 , b.KWD_NM as KWD_NM2  ,b.SEARCH_CNT as SEARCH_CNT2\n",
    "            from top20_5 as a right outer join top20_6 as b on a.KWD_NM = b.KWD_NM \n",
    "            \"\"\")\n",
    "\n",
    "top5_6.createOrReplaceTempView(\"top5_6\")\n",
    "\n",
    "p_top5_6=top5_6.toPandas()\n",
    "p_top5_6.to_csv(\"p_top5_6.csv\",mode=\"w\")\n",
    "\n",
    "top6_7=spark.sql(\"\"\"\n",
    "            select a.KWD_NM as KWD_NM1 ,a.SEARCH_CNT as SEARCH_CNT1 , b.KWD_NM as KWD_NM2  ,b.SEARCH_CNT as SEARCH_CNT2\n",
    "            from top20_6 as a right outer join top20_7 as b on a.KWD_NM = b.KWD_NM \n",
    "            \"\"\")\n",
    "\n",
    "top6_7.createOrReplaceTempView(\"top6_7\")\n",
    "\n",
    "p_top6_7=top6_7.toPandas()\n",
    "p_top6_7.to_csv(\"p_top6_7.csv\",mode=\"w\")\n",
    "\n",
    "top7_8=spark.sql(\"\"\"\n",
    "            select a.KWD_NM as KWD_NM1 ,a.SEARCH_CNT as SEARCH_CNT1 , b.KWD_NM as KWD_NM2  ,b.SEARCH_CNT as SEARCH_CNT2\n",
    "            from top20_7 as a right outer join top20_8 as b on a.KWD_NM = b.KWD_NM \n",
    "            \"\"\")\n",
    "\n",
    "top7_8.createOrReplaceTempView(\"top7_8\")\n",
    "\n",
    "p_top7_8=top7_8.toPandas()\n",
    "p_top7_8.to_csv(\"p_top7_8.csv\",mode=\"w\")\n",
    "\n",
    "top8_9=spark.sql(\"\"\"\n",
    "            select a.KWD_NM as KWD_NM1 ,a.SEARCH_CNT as SEARCH_CNT1 , b.KWD_NM as KWD_NM2  ,b.SEARCH_CNT as SEARCH_CNT2\n",
    "            from top20_8 as a right outer join top20_9 as b on a.KWD_NM = b.KWD_NM \n",
    "            \"\"\")\n",
    "\n",
    "top8_9.createOrReplaceTempView(\"top8_9\")\n",
    "\n",
    "p_top8_9=top8_9.toPandas()\n",
    "p_top8_9.to_csv(\"p_top8_9.csv\",mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5,6,7,8,9월에 갑자기 나타난 top20검색어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_5=spark.sql(\"\"\"\n",
    "            select KWD_NM2\n",
    "            from top4_5\n",
    "            where KWD_NM1 is null\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "outer_5.createOrReplaceTempView(\"outer_5\")\n",
    "p_outer_5=outer_5.toPandas()\n",
    "p_outer_5.to_csv(\"p_outer_5.csv\",mode=\"w\")\n",
    "\n",
    "outer_6=spark.sql(\"\"\"\n",
    "            select KWD_NM2\n",
    "            from top5_6\n",
    "            where KWD_NM1 is null\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "outer_6.createOrReplaceTempView(\"outer_6\")\n",
    "p_outer_6=outer_6.toPandas()\n",
    "p_outer_6.to_csv(\"p_outer_6.csv\",mode=\"w\")\n",
    "\n",
    "outer_7=spark.sql(\"\"\"\n",
    "            select KWD_NM2\n",
    "            from top6_7\n",
    "            where KWD_NM1 is null\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "outer_7.createOrReplaceTempView(\"outer_7\")\n",
    "p_outer_7=outer_7.toPandas()\n",
    "p_outer_7.to_csv(\"p_outer_7.csv\",mode=\"w\")\n",
    "\n",
    "outer_8=spark.sql(\"\"\"\n",
    "            select KWD_NM2\n",
    "            from top7_8\n",
    "            where KWD_NM1 is null\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "outer_8.createOrReplaceTempView(\"outer_8\")\n",
    "p_outer_8=outer_8.toPandas()\n",
    "p_outer_8.to_csv(\"p_outer_8.csv\",mode=\"w\")\n",
    "\n",
    "outer_9=spark.sql(\"\"\"\n",
    "            select KWD_NM2\n",
    "            from top8_9\n",
    "            where KWD_NM1 is null\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "outer_9.createOrReplaceTempView(\"outer_9\")\n",
    "p_outer_9=outer_9.toPandas()\n",
    "p_outer_9.to_csv(\"p_outer_9.csv\",mode=\"w\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5,6,7,8,9월에 갑자기 나타난 검색어의 검색량을 나타내는 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_search=spark.sql(\"\"\" select  SESS_DT,KWD_NM,SEARCH_CNT\n",
    "                            from search_2\n",
    "                           where KWD_NM = '크록스' or\n",
    "                               KWD_NM = '나이키운동화' or\n",
    "                               KWD_NM = '온앤온' or\n",
    "                               KWD_NM = '조르지오아르마니' or\n",
    "                               KWD_NM = '샌들' or\n",
    "                               KWD_NM = '린넨 원피스' or\n",
    "                               KWD_NM = '롱원피스' or\n",
    "                               KWD_NM = '핏플랍' or\n",
    "                               KWD_NM = '플라스틱아일랜드' or\n",
    "                               KWD_NM = '라코스테' or\n",
    "                               KWD_NM = '양산' or\n",
    "                               KWD_NM = '래쉬가드' or\n",
    "                               KWD_NM = '아쿠아슈즈' or\n",
    "                               KWD_NM = '롱패딩' or\n",
    "                               KWD_NM = '헤지스레이디스' or\n",
    "                               KWD_NM = '찰스앤키스' or\n",
    "                               KWD_NM = '샤넬' or\n",
    "                               KWD_NM = '나스' or\n",
    "                               KWD_NM = '쥬시쥬디' or\n",
    "                               KWD_NM = '뉴발란스키즈' or\n",
    "                               KWD_NM = '아디다스키즈' or\n",
    "                               KWD_NM = '정관장' or\n",
    "                               KWD_NM = '블루독베이비' or\n",
    "                               KWD_NM = '블루독' or\n",
    "                               KWD_NM = '베베드피노' or\n",
    "                               KWD_NM = '지오다노'   \n",
    "                               order by SESS_DT,KWD_NM\n",
    " \"\"\")\n",
    "\n",
    "outer_search.createOrReplaceTempView(\"outer_search\")\n",
    "p_outer_search=outer_search.toPandas()\n",
    "p_outer_search.to_csv(\"p_outer_search.csv\",mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# product 와 master를 조인해서 대분류를 끼워넣음(과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_table = spark.sql(\"\"\"select P.CLNT_ID,P.SESS_ID, P.PD_C, replace(P.PD_BUY_AM,',','') as PD_BUY_AM, M.CLAC1_NM\n",
    "            from product as P inner join master as M\n",
    "            where P.PD_C = M.PD_C\n",
    "            order by CLNT_ID\"\"\")\n",
    "\n",
    "middle_table.createOrReplaceTempView(\"middle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일별을 월별로 substr해서 고친것(과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = spark.sql(\"\"\"\n",
    "            select CLNT_ID , SESS_ID , substr(SESS_DT,3,4) as SESS_DT ,DVC_CTG_NM ,ZON_NM ,CITY_NM\n",
    "            from session\n",
    "            order by CLNT_ID\"\"\")\n",
    "\n",
    "\n",
    "month.createOrReplaceTempView(\"month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 월별 상품 대분류테이블(과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_month = spark.sql(\"\"\" select p.CLNT_ID ,p.SESS_ID , p.PD_BUY_AM,p.PD_C,mon.SESS_DT ,mon.DVC_CTG_NM ,mon.ZON_NM ,mon.CITY_NM ,P.CLAC1_NM\n",
    "                            from middle as p join month as mon using (CLNT_ID,SESS_ID) \n",
    "                            order by CLNT_ID\"\"\")\n",
    "\n",
    "\n",
    "pro_month.createOrReplaceTempView(\"pro_month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 월별 상품 대분류와 성별 연령을 합친 테이블(과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_month_cus = spark.sql(\"\"\" select p.CLNT_ID ,p.SESS_ID ,p.PD_C, p.PD_BUY_AM,p.SESS_DT ,p.DVC_CTG_NM ,p.ZON_NM ,p.CITY_NM ,p.CLAC1_NM, c.CLNT_GENDER ,c.CLNT_AGE\n",
    "                            from  pro_month as p join custom as c using (CLNT_ID)\n",
    "                            order by CLNT_ID\"\"\")\n",
    "\n",
    "\n",
    "pro_month_cus.createOrReplaceTempView(\"pro_month_cus\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 월별 거래량과 소비가격"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_month_count=spark.sql(\"\"\" select SESS_DT, count(CLAC1_NM), int(avg(PD_BUY_AM))\n",
    "                            from pro_month\n",
    "                            group by SESS_DT\n",
    "                            order by SESS_DT\"\"\") \n",
    "pro_month_count.createOrReplaceTempView(\"pro_month_count\")\n",
    "p_pro_month_count=pro_month_count.toPandas()\n",
    "p_pro_month_count.to_csv(\"p_pro_month_count.csv\",mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 월별 성별에 따른 거래량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_gen=spark.sql(\"\"\" select SESS_DT, CLNT_GENDER,count(CLNT_ID)\n",
    "                            from pro_month_cus\n",
    "                            group by SESS_DT,CLNT_GENDER\n",
    "                            order by SESS_DT\"\"\")\n",
    "\n",
    "mon_gen.createOrReplaceTempView(\"mon_gen\")\n",
    "p_mon_gen=mon_gen.toPandas()\n",
    "p_mon_gen.to_csv(\"p_mon_gen.csv\",mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 월별 대분류별 거래량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_mon_calc1= spark.sql(\"\"\" select CLAC1_NM,SESS_DT, count(CLAC1_NM)\n",
    "                            from pro_month\n",
    "                            group by CLAC1_NM,SESS_DT\n",
    "                            order by CLAC1_NM,SESS_DT\"\"\")\n",
    "pro_mon_calc1.createOrReplaceTempView(\"pro_mon_calc1\")\n",
    "p_pro_mon_calc1=pro_mon_calc1.toPandas()\n",
    "p_pro_mon_calc1.to_csv(\"p_pro_mon_calc1.csv\",mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pro_mon_clac1 = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패딩 연결 / 경량,숏,롱 패딩같은것 다 포함되어잇음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padingFilePath  = \"hdfs:///user/s21410781/finance/pading.csv\"\n",
    "pading = spark.read.csv(padingFilePath, header='true', sep=',')\n",
    "pading.createOrReplaceTempView(\"pading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패딩이라는 이름이있는 상품을 모두 찾아 count하는 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pading_join=spark.sql(\"\"\" select *\n",
    "                            from pro_month_cus as a join pading as b using(PD_C)\n",
    "                         \"\"\")\n",
    "\n",
    "pading_join.createOrReplaceTempView(\"pading_join\")\n",
    "\n",
    "mon_pading=spark.sql(\"\"\" select  SESS_DT, count(CLNT_ID), avg(PD_BUY_AM)\n",
    "                            from pading_join\n",
    "                         group by SESS_DT\n",
    "                            order by SESS_DT \"\"\")\n",
    "\n",
    "mon_pading.createOrReplaceTempView(\"mon_pading\")\n",
    "p_mon_pading=mon_pading.toPandas()\n",
    "p_mon_pading.to_csv(\"p_mon_pading.csv\",mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 롱패딩 연결후 롱패딩이라는 이름이 있는 \n",
    "# 상품을 모두 찾아 count하는 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "longpadingFilePath  = \"hdfs:///user/s21410781/finance/longpading.csv\"\n",
    "longpading = spark.read.csv(longpadingFilePath, header='true', sep=',')\n",
    "longpading.createOrReplaceTempView(\"longpading\")\n",
    "\n",
    "longpading_join=spark.sql(\"\"\" select *\n",
    "                            from pro_month_cus as a join longpading as b using(PD_C)\n",
    "                         \"\"\")\n",
    "\n",
    "longpading_join.createOrReplaceTempView(\"longpading_join\")\n",
    "\n",
    "mon_longpading=spark.sql(\"\"\" select  SESS_DT, count(CLNT_ID), round(avg(PD_BUY_AM),0)\n",
    "                            from longpading_join\n",
    "                         group by SESS_DT\n",
    "                            order by SESS_DT \"\"\")\n",
    "\n",
    "mon_longpading.createOrReplaceTempView(\"mon_longpading\")\n",
    "p_mon_longpading=mon_longpading.toPandas()\n",
    "p_mon_longpading.to_csv(\"p_mon_longpading.csv\",mode=\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
