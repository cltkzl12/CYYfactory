{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Basic\n",
    "### * Sequential Layer 의 주요 레이어 정리 \n",
    "#### [1]  Dense Layer : Fully Connected Layer, layer의 입력과 출력 사이에 있는 모든 뉴런이 서로 연결 되는 Layer\n",
    "#### [2]  Flatten Layer : 다차원(4차원)을 2차원으로 축소하여 FC Layer에 전달한다\n",
    "#### [3]  Conv2D Layer : 이미지 특징을 추출하는 Convolution Layer \n",
    "#### [4]  MaxPool2D Layer : 중요 데이터(값이 큰 데이터)를 subsampling 하는 Layer\n",
    "#### [5]  Dropout Layer : 학습시 신경망의 과적합을 막기위해 일부 뉴런을 제거하는 Layer\n",
    "\n",
    "#### https://greeksharifa.github.io/keras/2018/06/29/Keras-Tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras 를 이용한 XOR 네트워크\n",
    "\n",
    "# train data set \n",
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]   # (4,2)\n",
    "\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]     # (4,1)\n",
    "\n",
    "x_train = np.array(x_data,dtype=np.float32)\n",
    "y_train = np.array(y_data,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dense Layer 구현\n",
    "# tf.keras.layers.Dense(\n",
    "#     units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#     bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#     activity_regularizer=None, kernel_constraint=None, bias_constraint=None,\n",
    "#     **kwargs\n",
    "# )\n",
    "model = tf.keras.Sequential([\n",
    "    # (4,2) * (2,10) = (4,10)\n",
    "    # W : 2*10 = 20 , b : 10 ==> 학습 Parameter : 30\n",
    "    tf.keras.layers.Dense(units=10, activation='sigmoid', input_shape=(2,)), # (None,10)\n",
    "    \n",
    "    # (4,2) * (10,1) = (4,1)\n",
    "    # W : 10*1 = 10 , b : 1 ==> 학습 Parameter : 11\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid') # (None,1)   \n",
    "])\n",
    "\n",
    "model.summary() # # 각 layer의 정보를 출력\n",
    "\n",
    "# 옵션 설정\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.1), loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# loss 종류\n",
    "# mean_squared_error :  평균제곱 오차\n",
    "# binary_crossentropy : 이진분류 오차\n",
    "# categorical_crossentropy : 다중 분류 오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8719 - accuracy: 0.2500\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.7730 - accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 747us/step - loss: 0.7509 - accuracy: 0.2500\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7199 - accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.7295 - accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 756us/step - loss: 0.7525 - accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7289 - accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.7065 - accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7418 - accuracy: 0.5000\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7060 - accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7306 - accuracy: 0.2500\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.7119 - accuracy: 0.2500\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6908 - accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 749us/step - loss: 0.6818 - accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7073 - accuracy: 0.2500\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6723 - accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.6637 - accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6487 - accuracy: 0.7500\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6328 - accuracy: 0.7500\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.6201 - accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 498us/step - loss: 0.6000 - accuracy: 0.7500\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6005 - accuracy: 0.7500\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.5717 - accuracy: 0.7500\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.5591 - accuracy: 0.7500\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.5426 - accuracy: 0.7500\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.5608 - accuracy: 0.7500\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 498us/step - loss: 0.5254 - accuracy: 0.7500\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.5196 - accuracy: 0.7500\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.5171 - accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.5009 - accuracy: 0.7500\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.4837 - accuracy: 0.7500\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.4760 - accuracy: 0.7500\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.4639 - accuracy: 0.7500\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4713 - accuracy: 0.7500\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4489 - accuracy: 0.7500\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.4375 - accuracy: 0.7500\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 749us/step - loss: 0.4443 - accuracy: 0.7500\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.4168 - accuracy: 0.7500\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.4091 - accuracy: 0.7500\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 500us/step - loss: 0.3986 - accuracy: 0.7500\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.3923 - accuracy: 0.7500\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.3709 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.3635 - accuracy: 0.7500\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.3537 - accuracy: 0.7500\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 747us/step - loss: 0.3435 - accuracy: 0.7500\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.3188 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.3052 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.2943 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.2801 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2851 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 996us/step - loss: 0.2560 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.2307 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.2319 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2147 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.1961 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.1915 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.1737 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.1647 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.1546 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.1462 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.1369 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.1320 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.1214 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.1127 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.1126 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.0900 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 747us/step - loss: 0.0868 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0808 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0756 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0709 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0643 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 498us/step - loss: 0.0621 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.0587 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 498us/step - loss: 0.0555 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0534 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0513 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 498us/step - loss: 0.0495 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 498us/step - loss: 0.0472 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0444 - accuracy: 1.0000\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0423 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 749us/step - loss: 0.0413 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0391 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0378 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0361 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0352 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 747us/step - loss: 0.0336 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 498us/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 499us/step - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 748us/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 747us/step - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 995us/step - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 747us/step - loss: 0.0235 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2785ff58040>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 수행\n",
    "model.fit(x_train,y_train,epochs=100,batch_size=1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측\n",
    "preds = model.predict(x_train)\n",
    "np.round(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 998us/step - loss: 0.0227 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.022687405347824097, 1.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가 \n",
    "model.evaluate(x_train,y_train)\n",
    "# [0.022687405347824097, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 130       \n",
      "=================================================================\n",
      "Total params: 7,055\n",
      "Trainable params: 7,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# X,Y input data \n",
    "x_train = np.array([[1.0,   2.0,  3.0,  4.0,  5.0],\n",
    "                    [6.0,   7.0,  8.0,  9.0, 10.0],\n",
    "                    [11.0, 12.0, 13.0, 14.0, 15.0]])  # (3,5)\n",
    "                    \n",
    "                    \n",
    "y_train = np.array([[1.1,   2.2,  3.3,  4.4,  5.5],\n",
    "                    [6.1,   7.2,  8.3,  9.4, 10.5],\n",
    "                    [11.1, 12.2, 13.3, 14.4, 15.5]]) # (3,5)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # (3,1) * (1,100) = (3,100)   , Param :200 (W:1*100, b:100), (1 + 1) * 100 = 200\n",
    "    # tf.keras.layers.Dense(units=100, activation='relu', input_shape=(1,)), # input data의 shape을 (?,1) \n",
    "\n",
    "    # (3,5) * (5,100) = (3,100)   , Param :600 (W:5*100, b:100), (5 + 1) * (100) = 600\n",
    "    tf.keras.layers.Dense(units=100, activation='relu', input_shape=(5,)), # input data의 shape을 (?,5) \n",
    "\n",
    "    \n",
    "    # (3,100) * (100,50) = (3,50)   , Param : 5050 (W:100*50, b:50), (100+1)*50 = 5050\n",
    "    tf.keras.layers.Dense(units=50, activation='relu' ), \n",
    "    \n",
    "    # (3,50) * (50,25) = (3,25)   , Param : 1275 (W:50*25, b:25), (50 + 1)* 25 = 1275\n",
    "    tf.keras.layers.Dense(units=25, activation='relu' ),\n",
    "    \n",
    "    # (3,25) * (25,5) = (3,5)   , Param : 130 (W:25*5, b:5), (25 + 1) * 5 = 130\n",
    "    tf.keras.layers.Dense(units=5 , activation='relu' ),\n",
    "    \n",
    "#     # (3,5) * (5,1) = (3,1)   , Pram : 6  (W:5*1, b:1), (5 + 1) *1 = 6\n",
    "#     tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 86.7635\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 69.8507\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 995us/step - loss: 52.4968\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.9506\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 38.1061\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38.0145\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 995us/step - loss: 35.8517\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 978us/step - loss: 33.6248\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.6726\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.6053\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 23.2542\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 25.3115\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 23.0076\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 18.7607\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.8496\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15.0175\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 15.4816\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 16.3153\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.1365\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 17.2539\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 993us/step - loss: 16.4006\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.3136\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.8524\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 15.0868\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.4499\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.4486\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.0331\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.5195\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.2848\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 14.4185\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 988us/step - loss: 14.6270\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 14.6465\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.5704\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.5431\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 14.5444\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.4710\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 14.2883\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0851\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0270\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.1383\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.2226\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 14.1466\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 995us/step - loss: 14.0542\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 14.0451\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0750\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0904\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 14.0741\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 14.0526\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 14.0470\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0376\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.9974\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 13.9410\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 13.9105\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.9217\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 13.9483\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 994us/step - loss: 13.9572\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.9472\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 13.9283\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 13.9103\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.8993\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 13.9016\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 13.9019\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 13.8951\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.8827\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 13.8698\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 13.8602\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.8480\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.8316\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.8415\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 13.8143\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 993us/step - loss: 13.8219\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.8158\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.7920\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 13.7907\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 13.7806\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 13.7647\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 13.7642\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 991us/step - loss: 13.7525\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 13.7389\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 13.7389\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13.7286\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13.7174\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 13.7170\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.7107\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.7105\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 13.7093\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 13.7054\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 13.7097\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 13.7077\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.7107\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.7103\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.7097\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 13.7106\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 995us/step - loss: 13.7082\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 13.7090\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.7061\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.7062\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 13.7054\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.7044\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 13.7042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2786323bbe0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98594826,  0.        ,  3.3009012 ,  4.4354005 ,  5.4840903 ],\n",
       "       [ 6.053206  ,  0.        ,  8.268296  ,  9.388879  , 10.451506  ],\n",
       "       [11.146271  ,  0.        , 13.302449  , 14.4046    , 15.545564  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 999us/step - loss: 13.7030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.702960014343262"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] Flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28, 10)\n",
      "(1000, 7840)\n"
     ]
    }
   ],
   "source": [
    "# 4차원 데이터를 2차원으로 축소하여 FC Layer에 전달\n",
    "data = np.arange(1000*28*28*10).reshape(1000,28,28,10).astype(np.float32)\n",
    "print(data.shape) # (None, 28, 28, 10)\n",
    "layer = tf.keras.layers.Flatten()\n",
    "outputs = layer(data)\n",
    "print(outputs.shape) # (None, 28*28*10) = (None,7840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28)\n",
      "(1000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 3차원 데이터를 2차원으로 축소하여 FC Layer에 전달\n",
    "data = np.arange(1000*28*28).reshape(1000,28,28).astype(np.float32)\n",
    "print(data.shape) # (None, 28, 28)\n",
    "layer = tf.keras.layers.Flatten()\n",
    "outputs = layer(data)\n",
    "print(outputs.shape) # (None, 28*28) = (None,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 2차원 데이터는 그대로 2차원으로 FC Layer에 전달\n",
    "data = np.arange(28*28).reshape(28,28).astype(np.float32)\n",
    "print(data.shape) # (28, 28)\n",
    "layer = tf.keras.layers.Flatten()\n",
    "outputs = layer(data)\n",
    "print(outputs.shape) # (28,28) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
    "# The Dropout layer randomly sets input units to 0 with a frequency of rate \n",
    "# at each step during training time, which helps prevent overfitting. \n",
    "# Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
    "\n",
    "# Note that the Dropout layer only applies when training is set to True such that\n",
    "# no values are dropped during inference. When using model.fit, training will be\n",
    "# appropriately set to True automatically, and in other contexts, you can set \n",
    "# the kwarg explicitly to True when calling the layer.\n",
    "\n",
    "# (This is in contrast to setting trainable=False for a Dropout layer.\n",
    "# trainable does not affect the layer's behavior, as Dropout does not have any \n",
    "# variables/weights that can be frozen during training.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [2. 3.]\n",
      " [4. 5.]\n",
      " [6. 7.]\n",
      " [8. 9.]]\n",
      "tf.Tensor(\n",
      "[[ 0.         0.       ]\n",
      " [ 2.857143   4.285714 ]\n",
      " [ 5.714286   7.1428576]\n",
      " [ 8.571428  10.       ]\n",
      " [11.428572   0.       ]], shape=(5, 2), dtype=float32)\n",
      "[[ 0.         1.4285715]\n",
      " [ 2.857143   4.285714 ]\n",
      " [ 5.714286   7.142857 ]\n",
      " [ 8.571428  10.       ]\n",
      " [11.428572  12.857143 ]]\n"
     ]
    }
   ],
   "source": [
    "# keras dropout\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "data = np.arange(10).reshape(5,2).astype(np.float32)\n",
    "print(data)\n",
    "\n",
    "layer = tf.keras.layers.Dropout(0.3,input_shape=(2,))\n",
    "\n",
    "outputs = layer(data,training=True)\n",
    "# model.fit()호출 시는  학습 모드로 training=True가 되어 dropuout 적용\n",
    "# model.evaluate() 호출 시는 예측 모드로 False가 되어 dropuout이 수행되지 않음\n",
    "print(outputs) # 데이터의 30%는 0으로 나머지 데이터는  1/(1-0.3)으로 곱하여 scaled up된다\n",
    "print(data*1/(1 - 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[ 0.       ,  0.       ],\n",
       "       [ 2.857143 ,  4.285714 ],\n",
       "       [ 5.714286 ,  7.1428576],\n",
       "       [ 8.571428 , 10.       ],\n",
       "       [11.428572 ,  0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구버전 dropout : 사용이 번거로움\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "data = np.arange(10).reshape(5,2).astype(np.float32)\n",
    "\n",
    "def layer1(X,dr_rate):\n",
    "    return tf.nn.dropout(X,rate=dr_rate)\n",
    "\n",
    "layer1(data,dr_rate=0.3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
